{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "\n",
    "import dataset_class\n",
    "import torch_net_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Utilizing CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizing CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create train, val, test index list dictionary\n",
    "\"\"\"\n",
    "\n",
    "def create_index_split(dataset_size=10, split_ratio={\"train\":0.0, \"val\":0.0, \"test\":0.1}, shuffle=False, np_random_seed=42):\n",
    "    \n",
    "    ratio_norm = sum(split_ratio.values())\n",
    "    \"\"\"\n",
    "    if ratio_norm is not 1.0:\n",
    "        train_ratio = split_ratio[\"train\"]/ratio_norm\n",
    "        val_ratio = split_ratio[\"val\"]/ratio_norm\n",
    "        test_ratio = split_ratio[\"val\"]/ratio_norm\n",
    "    \"\"\"\n",
    "    \n",
    "    train_ratio = split_ratio[\"train\"]\n",
    "    val_ratio = split_ratio[\"val\"]\n",
    "    test_ratio = split_ratio[\"test\"]\n",
    "    \n",
    "    indices = list(range(dataset_size))\n",
    "    if shuffle is True:\n",
    "        np.random.seed(np_random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    \n",
    "    split_indices = {}\n",
    "    split_indices[\"train\"] = indices[:int(train_ratio*dataset_size)]\n",
    "    split_indices[\"val\"] = indices[int(train_ratio*dataset_size) : int((train_ratio+val_ratio)*dataset_size)]\n",
    "    split_indices[\"test\"] = indices[int((train_ratio+val_ratio)*dataset_size) : int((train_ratio+val_ratio+test_ratio)*dataset_size)]\n",
    "    \n",
    "    #print(split_indices)\n",
    "    return split_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function for training loop\n",
    "\"\"\"\n",
    "\n",
    "def step(model, input_data, batch_size, loss_func, optimizer, epoch, batch_nr, device, log_file, mode=\"val\"):\n",
    "        if mode == \"train\":\n",
    "            model.train()\n",
    "        elif mode == \"val\":\n",
    "            model.eval()\n",
    "        \n",
    "        #input data splitted into features and labels\n",
    "        feat_batch = input_data[0].to(device)\n",
    "        label_batch = input_data[1].to(device)\n",
    "        \n",
    "        input_size = model.get_input_size()\n",
    "        output_size = model.get_output_size()\n",
    "    \n",
    "        \"\"\"ALWAYS SET GRADIENT TO ZERO  FOR STANDARD NN (NOT RNNs)\"\"\"\n",
    "        model.zero_grad()\n",
    "        #optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        the peculiar shape (-1, sample_size) is needed, because an entire mini batch is passed on to the network\n",
    "        initially it is not clear how large such a mini batch is\n",
    "        the -1 acts as a placeholder in order to keep the number of processed items in one mini batch flexible\n",
    "        \"\"\"\n",
    "        #print(\"Input {}, batch: {} : {}\".format(epoch, batch_nr, feat_batch))\n",
    "        #log_file.write(\"Input {}, batch: {} : {}\\n\".format(epoch, batch_nr, feat_batch))\n",
    "        \n",
    "        #print(\"Reshaping to {}\".format( feat_batch.view(-1, input_size).double().size() ))\n",
    "        output = model(feat_batch.view(-1, input_size).float())\n",
    "        output = output.to(device)\n",
    "    \n",
    "        #print(\"Output {}, batch: {} : {}\".format(epoch, batch_nr, output))\n",
    "        #print(\"Label {}, batch: {} : {}\".format(epoch, batch_nr, label_batch))\n",
    "        #log_file.write(\"Output {}, batch: {} : {}\\n\".format(epoch, batch_nr, output))\n",
    "        \n",
    "        #print(\"Input shape {}\".format(feat_batch.size()))\n",
    "        #print(\"Label shape {}\".format(label_batch.size()))\n",
    "        #print(\"Output shape {}\".format(output.size()))\n",
    "    \n",
    "        #print(\"Feeding forward epoch: {}, batch: {}\".format(epoch, batch_nr))\n",
    "        #log_file.write(\"Feeding forward epoch: {}, batch: {}\\n\".format(epoch, batch_nr))\n",
    "    \n",
    "        #print(\"Calculating \" + mode + \" loss epoch: {}, batch: {}\".format(epoch, batch_nr))\n",
    "        #log_file.write(\"Calculating \" + mode + \" loss epoch: {}, batch: {}\\n\".format(epoch, batch_nr))\n",
    "        \"\"\"MSE\"\"\"\n",
    "        #loss = loss_func(output.view(-1, output_size).float(), label_batch.view(-1, output_size).float())\n",
    "        \"\"\"Cross Entropy\"\"\"\n",
    "        loss = loss_func(output.view(-1, output_size).float(), label_batch.view(-1).long())\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            #print(\"epoch: {}, batch: {}, loss: {}\".format(epoch, batch_nr, loss.item()))\n",
    "            #print(\"Performing backprop ...\")\n",
    "            #log_file.write(\"epoch: {}, batch: {}, loss: {}\\n\".format(epoch, batch_nr, loss.item()))\n",
    "            #log_file.write(\"Performing backprop ...\\n\")\n",
    "            loss.backward()\n",
    "    \n",
    "            #print(\"Adjusting weights ...\")\n",
    "            #log_file.write(\"Adjusting weights ...\\n\")\n",
    "            optimizer.step()\n",
    "    \n",
    "        return loss, output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
      "['label']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data preparation\n",
    "\"\"\"\n",
    "\n",
    "dataset_file_path = \"ionosphere_train.csv\"\n",
    "test_dataset_file_path=\"ionosphere_test.csv\"\n",
    "dataset = dataset_class.dataset(dataset_file_path)\n",
    "test_dataset = dataset_class.dataset(test_dataset_file_path)\n",
    "#selected_pred_attributes = pd.read_csv('amazon_pred_attributes_var_corr.csv', header=None)[0].tolist()\n",
    "#target_attributes = pd.read_csv('amazon_target_attribute.csv', header=None)[0].tolist()\n",
    "selected_pred_attributes = [ '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
    "       '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
    "target_attributes = [\"label\"]\n",
    "output_attributes = [\"good\", \"bad\"]\n",
    "print(selected_pred_attributes)\n",
    "print(target_attributes)\n",
    "#print(list(dataset.df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
      "['label']\n"
     ]
    }
   ],
   "source": [
    "pred_attributes = selected_pred_attributes\n",
    "\n",
    "dataset.set_pred_attributes(pred_attributes)\n",
    "dataset.set_target_attribute(target_attributes)\n",
    "test_dataset.set_pred_attributes(pred_attributes)\n",
    "test_dataset.set_target_attribute(target_attributes)\n",
    "print(pred_attributes)\n",
    "print(target_attributes)\n",
    "#dataset.df[pred_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.477824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label\n",
       "count  280.000000\n",
       "mean     0.650000\n",
       "std      0.477824\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[target_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.0</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642006</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>0.613042</td>\n",
       "      <td>0.124715</td>\n",
       "      <td>0.558642</td>\n",
       "      <td>0.100720</td>\n",
       "      <td>0.514151</td>\n",
       "      <td>0.178432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401549</td>\n",
       "      <td>-0.048062</td>\n",
       "      <td>0.546989</td>\n",
       "      <td>-0.057030</td>\n",
       "      <td>0.376318</td>\n",
       "      <td>-0.047793</td>\n",
       "      <td>0.349299</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.358110</td>\n",
       "      <td>0.006360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.314340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507474</td>\n",
       "      <td>0.444380</td>\n",
       "      <td>0.520414</td>\n",
       "      <td>0.455321</td>\n",
       "      <td>0.493897</td>\n",
       "      <td>0.514517</td>\n",
       "      <td>0.522983</td>\n",
       "      <td>0.485672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584663</td>\n",
       "      <td>0.501739</td>\n",
       "      <td>0.530014</td>\n",
       "      <td>0.554377</td>\n",
       "      <td>0.590581</td>\n",
       "      <td>0.503374</td>\n",
       "      <td>0.591181</td>\n",
       "      <td>0.513241</td>\n",
       "      <td>0.528868</td>\n",
       "      <td>0.455715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>-0.082590</td>\n",
       "      <td>0.435122</td>\n",
       "      <td>-0.023300</td>\n",
       "      <td>0.256398</td>\n",
       "      <td>-0.054445</td>\n",
       "      <td>0.137793</td>\n",
       "      <td>-0.044605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.279763</td>\n",
       "      <td>0.332323</td>\n",
       "      <td>-0.336245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.240383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.234442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.188065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.880940</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.833680</td>\n",
       "      <td>0.023915</td>\n",
       "      <td>0.748205</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.692890</td>\n",
       "      <td>0.023530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559255</td>\n",
       "      <td>-0.008660</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>-0.014165</td>\n",
       "      <td>0.507815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432665</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.156713</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.338090</td>\n",
       "      <td>0.968035</td>\n",
       "      <td>0.364705</td>\n",
       "      <td>0.956382</td>\n",
       "      <td>0.541510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916347</td>\n",
       "      <td>0.160588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162285</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>0.129445</td>\n",
       "      <td>0.880257</td>\n",
       "      <td>0.214865</td>\n",
       "      <td>0.839647</td>\n",
       "      <td>0.155463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0      1           2           3           4           5  \\\n",
       "count  280.000000  280.0  280.000000  280.000000  280.000000  280.000000   \n",
       "mean     0.889286    0.0    0.642006    0.005401    0.613042    0.124715   \n",
       "std      0.314340    0.0    0.507474    0.444380    0.520414    0.455321   \n",
       "min      0.000000    0.0   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%      1.000000    0.0    0.473603   -0.082590    0.435122   -0.023300   \n",
       "50%      1.000000    0.0    0.880940    0.007075    0.833680    0.023915   \n",
       "75%      1.000000    0.0    1.000000    0.156713    1.000000    0.338090   \n",
       "max      1.000000    0.0    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "                6           7           8           9  ...          24  \\\n",
       "count  280.000000  280.000000  280.000000  280.000000  ...  280.000000   \n",
       "mean     0.558642    0.100720    0.514151    0.178432  ...    0.401549   \n",
       "std      0.493897    0.514517    0.522983    0.485672  ...    0.584663   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
       "25%      0.256398   -0.054445    0.137793   -0.044605  ...    0.000000   \n",
       "50%      0.748205    0.011710    0.692890    0.023530  ...    0.559255   \n",
       "75%      0.968035    0.364705    0.956382    0.541510  ...    0.916347   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "               25          26          27          28          29          30  \\\n",
       "count  280.000000  280.000000  280.000000  280.000000  280.000000  280.000000   \n",
       "mean    -0.048062    0.546989   -0.057030    0.376318   -0.047793    0.349299   \n",
       "std      0.501739    0.530014    0.554377    0.590581    0.503374    0.591181   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%     -0.279763    0.332323   -0.336245    0.000000   -0.240383    0.000000   \n",
       "50%     -0.008660    0.723380   -0.014165    0.507815    0.000000    0.451060   \n",
       "75%      0.160588    1.000000    0.162285    0.904700    0.129445    0.880257   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               31          32          33  \n",
       "count  280.000000  280.000000  280.000000  \n",
       "mean     0.013766    0.358110    0.006360  \n",
       "std      0.513241    0.528868    0.455715  \n",
       "min     -1.000000   -1.000000   -1.000000  \n",
       "25%     -0.234442    0.000000   -0.188065  \n",
       "50%      0.000000    0.432665    0.000000  \n",
       "75%      0.214865    0.839647    0.155463  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[pred_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data scaling'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Data scaling\"\"\"\n",
    "\n",
    "#dataset.min_max_scaling(attributes=pred_attributes)\n",
    "#dataset.z_score_standardization(attributes=pred_attributes)\n",
    "#test_dataset.z_score_standardization(attributes=pred_attributes)\n",
    "#dataset.z_score_standardization(attributes=pred_attributes[3:])\n",
    "#dataset.min_max_scaling(attributes=target_attributes)\n",
    "#scaling_func=lambda x: x/9.\n",
    "#inv_scaling_func = lambda x: 9.*x\n",
    "#dataset.custom_scaling(attributes=target_attributes, scaling_func=scaling_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.df[pred_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.df[target_attributes].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparameter_selector = []\\nparameter_option = {\"batch_size\":16,\"lr\":0.0001, \"k_fold\":1,\"hidden_layers\":1,\"layer_width\":4,\"act_func\":torch.sigmoid,\"loss_func\":F.mse_loss,\"dropout\":None,\"p_dropout\":0.4,\"bias\":True,\"optimizer\":optim.Adam}\\nparameter_option[\"hidden_layers\"] = 3\\nparameter_option[\"layer_width\"] = 10\\nparameter_options[\"act_func\"] = [torch.relu]\\nparameter_options[\"net_struct\"] = fixed_net_struct\\nparameter_selector.append(parameter_option)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "\"\"\"\n",
    "parameter_options = {}\n",
    "parameter_options[\"act_func\"] = [torch.relu, torch.sigmoid, , torch.tanh]\n",
    "parameter_options[\"loss_func\"] = [torch.sigmoid, torch.relu, torch.tanh]\n",
    "parameter_options[\"dropout\"] = [nn.Dropout, None]\n",
    "parameter_options[\"optimizer\"] = [optim.Adam, optim.SGD]\n",
    "parameter_options[\"val\"] = [\"kfold\", \"holdout\"]\n",
    "parameter_options[\"task\"] = [\"classification\", \"regression\"] \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"the parameter options dictionary records which hyperparameters to use\"\"\"\n",
    "parameter_options = {}\n",
    "\n",
    "parameter_options[\"task\"] = [\"classification\"] \n",
    "\n",
    "parameter_options[\"val\"] = [\"holdout\"]\n",
    "parameter_options[\"split\"] = [0.7]\n",
    "#parameter_options[\"val\"] = [\"kfold\"]\n",
    "#parameter_options[\"k_fold\"] = [10]\n",
    "\n",
    "parameter_options[\"batch_size\"] = [1]\n",
    "parameter_options[\"lr\"] = [0.001]\n",
    "\n",
    "parameter_options[\"hidden_layers\"] = [3]\n",
    "parameter_options[\"layer_width\"] = [9]\n",
    "parameter_options[\"bias\"] = [True]\n",
    "parameter_options[\"act_func\"] = [torch.relu]\n",
    "\n",
    "#parameter_options[\"loss_func\"] = [nn.MSELoss]\n",
    "parameter_options[\"loss_func\"] = [nn.CrossEntropyLoss]\n",
    "parameter_options[\"optimizer\"] = [optim.Adam]\n",
    "\n",
    "parameter_options[\"dropout\"] = [None]\n",
    "parameter_options[\"p_dropout\"] = [0.1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_epochs = [16, 32, 48, 64, 86, 100]\n",
    "save_state_epochs = [100]\n",
    "\n",
    "\"\"\"\n",
    "Create cartesian product of all possible parameter combinations\n",
    "\"\"\"\n",
    "def cart_product(dict_options):\n",
    "    return ( dict(zip(dict_options.keys(), values)) for values in itertools.product(*dict_options.values()) )\n",
    "\n",
    "parameter_selector = cart_product(parameter_options)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Manually set network structure\"\"\"\n",
    "\"\"\"\n",
    "    This list can be loaded into the constructor of the Net neural network class, to automatically generate the network structure\n",
    "    type = pointer to the layer function'\n",
    "    layer_pars = parameters which must be given to the layer function in order to initialize it\n",
    "    act_func = activation function to be applied directly after feeding to the corresponding layer\n",
    "    dropout = certain neurons cna be dropped out if specified\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "fixed_net_struct = []\n",
    "layer = nn.Linear\n",
    "input_size = len(pred_attributes)\n",
    "target_size = len(target_attributes)\n",
    "output_size = len(output_attributes)\n",
    "fixed_net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": input_size, \"out_features\": 20}, \"act_func\": act_func, \"bias\": bias} )\n",
    "if dropout is not None:    \n",
    "    fixed_net_struct.append( {\"type\": dropout, \"layer_pars\": {\"p\": p }} )\n",
    "fixed_net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": 20, \"out_features\": 20}, \"act_func\": act_func, \"bias\": bias} )\n",
    "fixed_net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": 20, \"out_features\": output_size}, \"bias\": bias} )\n",
    "\"\"\"\n",
    "\"\"\"If required create list of parameters manually\"\"\"\n",
    "\"\"\"\n",
    "parameter_selector = []\n",
    "parameter_option = {\"batch_size\":16,\"lr\":0.0001, \"k_fold\":1,\"hidden_layers\":1,\"layer_width\":4,\"act_func\":torch.sigmoid,\"loss_func\":F.mse_loss,\"dropout\":None,\"p_dropout\":0.4,\"bias\":True,\"optimizer\":optim.Adam}\n",
    "parameter_option[\"hidden_layers\"] = 3\n",
    "parameter_option[\"layer_width\"] = 10\n",
    "parameter_options[\"act_func\"] = [torch.relu]\n",
    "parameter_options[\"net_struct\"] = fixed_net_struct\n",
    "parameter_selector.append(parameter_option)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "{'in_features': 34, 'out_features': 9}\n",
      "{'in_features': 9, 'out_features': 9}\n",
      "{'in_features': 9, 'out_features': 9}\n",
      "{'in_features': 9, 'out_features': 2}\n",
      "Linear(in_features=34, out_features=9, bias=True)\n",
      "Linear(in_features=9, out_features=9, bias=True)\n",
      "Linear(in_features=9, out_features=9, bias=True)\n",
      "Linear(in_features=9, out_features=2, bias=True)\n",
      "mean epoch loss: 0.5618862923310728\n",
      "mean epoch loss: 0.4465485393088691\n",
      "mean epoch loss: 0.35171291247314335\n",
      "mean epoch loss: 0.27947429035391125\n",
      "mean epoch loss: 0.2259673774242401\n",
      "mean epoch loss: 0.19040576475007193\n",
      "mean epoch loss: 0.15875034429589097\n",
      "mean epoch loss: 0.1358966778735725\n",
      "mean epoch loss: 0.12546593741494783\n",
      "mean epoch loss: 0.10710330094609942\n",
      "mean epoch loss: 0.10138578408834886\n",
      "mean epoch loss: 0.08464654398207762\n",
      "mean epoch loss: 0.0933054931917969\n",
      "mean epoch loss: 0.07265023005251982\n",
      "mean epoch loss: 0.060092106765630295\n",
      "mean epoch loss: 0.058970279535468746\n",
      "mean epoch loss: 0.05590557778368191\n",
      "mean epoch loss: 0.0439882080773918\n",
      "mean epoch loss: 0.04179645618613886\n",
      "mean epoch loss: 0.03307210699636109\n",
      "mean epoch loss: 0.04501231166781211\n",
      "mean epoch loss: 0.049037287733992754\n",
      "mean epoch loss: 0.02984710341813613\n",
      "mean epoch loss: 0.024095649621924575\n",
      "mean epoch loss: 0.023084938526153564\n",
      "mean epoch loss: 0.022829177732370337\n",
      "mean epoch loss: 0.01726017861950154\n",
      "mean epoch loss: 0.01693900659376261\n",
      "mean epoch loss: 0.016140747435238897\n",
      "mean epoch loss: 0.010485632991304204\n",
      "mean epoch loss: 0.015602788450766583\n",
      "mean epoch loss: 0.015979865984040862\n",
      "mean epoch loss: 0.010115394330754573\n",
      "mean epoch loss: 0.012290194630622864\n",
      "mean epoch loss: 0.01638472901315105\n",
      "mean epoch loss: 0.024165137081730122\n",
      "mean epoch loss: 0.008857044638419638\n",
      "mean epoch loss: 0.009694921118872506\n",
      "mean epoch loss: 0.010814499186009777\n",
      "mean epoch loss: 0.005465899194989886\n",
      "mean epoch loss: 0.003963457686560494\n",
      "mean epoch loss: 0.004860508806851445\n",
      "mean epoch loss: 0.003944186227662223\n",
      "mean epoch loss: 0.0026427476989979645\n",
      "mean epoch loss: 0.0032562513132484592\n",
      "mean epoch loss: 0.0020618694169180734\n",
      "mean epoch loss: 0.002051025629043579\n",
      "mean epoch loss: 0.002026399787591428\n",
      "mean epoch loss: 0.0015827678904241445\n",
      "mean epoch loss: 0.07847802888373939\n",
      "mean epoch loss: 0.03547017854087207\n",
      "mean epoch loss: 0.004510566896321822\n",
      "mean epoch loss: 0.0029208100571924325\n",
      "mean epoch loss: 0.0021273518095211107\n",
      "mean epoch loss: 0.0018921184296510657\n",
      "mean epoch loss: 0.001748640318306125\n",
      "mean epoch loss: 0.001495224480726281\n",
      "mean epoch loss: 0.0015051492622920445\n",
      "mean epoch loss: 0.0012322049968096676\n",
      "mean epoch loss: 0.0011241691453116281\n",
      "mean epoch loss: 0.0011385770476594263\n",
      "mean epoch loss: 0.001095251769435649\n",
      "mean epoch loss: 0.0009082203008690659\n",
      "mean epoch loss: 0.0008885671897810333\n",
      "mean epoch loss: 0.0008205108496607567\n",
      "mean epoch loss: 0.0007673313423078887\n",
      "mean epoch loss: 0.0007063278130122594\n",
      "mean epoch loss: 0.0007917467428713429\n",
      "mean epoch loss: 0.0011207631656101771\n",
      "mean epoch loss: 0.011984637501288434\n",
      "mean epoch loss: 0.08056370822750793\n",
      "mean epoch loss: 0.01938816600916337\n",
      "mean epoch loss: 0.01000302969192972\n",
      "mean epoch loss: 0.0012778645875502605\n",
      "mean epoch loss: 0.0010991497915618274\n",
      "mean epoch loss: 0.0010127875269675742\n",
      "mean epoch loss: 0.0009486687426664391\n",
      "mean epoch loss: 0.000850196395601545\n",
      "mean epoch loss: 0.0007930811570615185\n",
      "mean epoch loss: 0.0007391693640728386\n",
      "mean epoch loss: 0.0007061599468698307\n",
      "mean epoch loss: 0.0006590436915962063\n",
      "mean epoch loss: 0.0005958956115099848\n",
      "mean epoch loss: 0.0005609234985040159\n",
      "mean epoch loss: 0.0005149677091715288\n",
      "mean epoch loss: 0.0004948243802907515\n",
      "mean epoch loss: 0.0004783807968606754\n",
      "mean epoch loss: 0.00046809230531964986\n",
      "mean epoch loss: 0.0004257705746864786\n",
      "mean epoch loss: 0.00039744498778362665\n",
      "mean epoch loss: 0.0003837882256021305\n",
      "mean epoch loss: 0.0003567921872041663\n",
      "mean epoch loss: 0.0003195855082297812\n",
      "mean epoch loss: 0.0003407670527088399\n",
      "mean epoch loss: 0.0002775569351351991\n",
      "mean epoch loss: 0.0002735463940367407\n",
      "mean epoch loss: 0.00026382110556777644\n",
      "mean epoch loss: 0.00023477661366365393\n",
      "mean epoch loss: 0.0002415253191578145\n",
      "mean epoch loss: 0.0001952915775532625\n",
      "saved model from epoch 99\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "All errors from each validation epoch will be saved\n",
    "this list contains them\n",
    "Saving them in memory is not always viable, which is why they are also saved on the hard drive\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "model_errors = []\n",
    "\"\"\"\n",
    "All errors from each validation epoch will be saved as a file\n",
    "this list contains the path to these files\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#fourth is prediction or label \n",
    "model_errors_path_list = []\n",
    "\n",
    "\"\"\"\n",
    "The average training loss for each epoch is recorded here\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "loss_curves = []\n",
    "loss_path_list = []\n",
    "\n",
    "\"\"\"\n",
    "States of the neural network are saved as a file\n",
    "filename is part of this list\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "saved_states_file_path_list = []\n",
    "\n",
    "\"\"\"Perform training for each parameter option in parameter_selector\"\"\"\n",
    "par_i = 0\n",
    "for hyper_parameters in parameter_selector:\n",
    "    \n",
    "    epochs = max(val_epochs)\n",
    "    input_size = len(pred_attributes)\n",
    "    target_size = len(target_attributes)\n",
    "    output_size = len(output_attributes)\n",
    "    \n",
    "    lr=hyper_parameters[\"lr\"]\n",
    "    batch_size = hyper_parameters[\"batch_size\"]\n",
    "    act_func = hyper_parameters[\"act_func\"]\n",
    "    loss_func = hyper_parameters[\"loss_func\"]()\n",
    "    #loss_func = hyper_parameters[\"loss_func\"]\n",
    "    dropout = hyper_parameters[\"dropout\"]\n",
    "    p = hyper_parameters[\"p_dropout\"]\n",
    "    bias = hyper_parameters[\"bias\"]\n",
    "\n",
    "    lw = hyper_parameters[\"layer_width\"]\n",
    "    hl = hyper_parameters[\"hidden_layers\"]\n",
    "    layer = nn.Linear\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    If a network structure is fixed beforehand, construct this\n",
    "    Otherwise create a \"rectangular\" fully connected feedforward network, with a fixed number of hidden layers and neurons per layer\n",
    "    \"\"\"\n",
    "    net_struct = []\n",
    "    if \"net_struct\" in hyper_parameters.keys():\n",
    "        net_struct = hyper_parameters[\"net_struct\"]\n",
    "    else:\n",
    "        net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": input_size, \"out_features\": lw}, \"act_func\": act_func, \"bias\": bias} )\n",
    "        if dropout is not None:    \n",
    "            net_struct.append( {\"type\": dropout, \"layer_pars\": {\"p\": p }} )\n",
    "        if hl > 1:\n",
    "            for num_layer in range(hl-1):\n",
    "                    net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": lw, \"out_features\": lw}, \"act_func\": act_func, \"bias\": bias} )\n",
    "        net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": lw, \"out_features\": output_size}, \"bias\": bias} )\n",
    "    \n",
    "    \"\"\"Choose between holdout validation and k-fold cross validation\"\"\"\n",
    "    \"\"\"!!!HOLDOUT!!!\"\"\"\n",
    "    \n",
    "    split_ratio = {\"train\" : 0.7, \"val\" : 0.3, \"test\" : 0.0}\n",
    "    #split_ratio = {\"train\" : 0.0, \"val\" : 1.0, \"test\" : 0.0}\n",
    "    #split_ratio = {\"train\" : 1.0, \"val\" : 0.0, \"test\" : 0.0}\n",
    "    split_indices = create_index_split(dataset_size=dataset.get_length(), split_ratio=split_ratio, shuffle=True, np_random_seed=42)\n",
    "    #test_split_ratio = {\"train\" : 0.0, \"val\" : 1.0, \"test\" : 0.0}\n",
    "    #split_indices[\"val\"] = create_index_split(dataset_size=test_dataset.get_length(), split_ratio=test_split_ratio, shuffle=True, np_random_seed=42)[\"val\"]\n",
    "    \n",
    "    \n",
    "    \"\"\"!!!HOLDOUT!!!\"\"\"\n",
    "    \n",
    "    indices = []\n",
    "    indices.append([split_indices[\"train\"], split_indices[\"val\"]])\n",
    "    #indices.append([[0], split_indices[\"val\"]])\n",
    "    \n",
    "    \"\"\"!!!KFOLD!!!\"\"\"\n",
    "    \"\"\"\n",
    "    n_splits = hyper_parameters[\"k_fold\"]\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    indices = kf.split(dataset.df)\n",
    "    \"\"\"\n",
    "    #if dataset is split into train test and val, use only train and val for KFold\n",
    "    #split_indices[\"train_val\"] = split_indices[\"train\"] + split_indices[\"val\"]\n",
    "    #indices = kf.split(dataset.df.iloc[split_indices[\"train_val\"]])\n",
    "    \n",
    "    \"\"\"\n",
    "    if hyper_parameters[\"val\"] == \"holdout\":\n",
    "        split = hyper_parameters[\"split\"]\n",
    "        split_ratio = {\"train\" : split, \"val\" : (1-split), \"test\" : 0.0}\n",
    "        split_indices = create_index_split(dataset_size=dataset.get_length(), split_ratio=split_ratio, shuffle=True, np_random_seed=42)\n",
    "        indices.append([split_indices[\"train\"], split_indices[\"val\"]])\n",
    "        print(len(indices))\n",
    "    elif hyper_parameters[\"val\"]:\n",
    "        n_splits = hyper_parameters[\"k_fold\"]\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        indices = kf.split(dataset.df)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "    par_dir = str(\"par_{}\".format(par_i))\n",
    "    \n",
    "    fold_model_errors = []\n",
    "    fold_model_errors_path_list = []\n",
    "    fold_saved_states_path_list = []\n",
    "    par_loss_curves = []\n",
    "    n_split = 0\n",
    "    for fold_indices in indices:\n",
    "        \n",
    "        train_indices = fold_indices[0]\n",
    "        val_indices = fold_indices[1]\n",
    "        \n",
    "        fold_loss_curve = []\n",
    "        print(\"fold {}\".format(n_split))\n",
    "        fold_dir = par_dir + \"/\" + \"fold_{}\".format(str(n_split))\n",
    "        try:\n",
    "            os.makedirs(fold_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        net = torch_net_class.Net(net_struct)\n",
    "        net.init_weights(torch.nn.init.xavier_normal_)\n",
    "        net.set_batch_size(batch_size)\n",
    "        #net.cuda()\n",
    "        net.to(device)\n",
    "        \n",
    "        net_parameters = net.parameters()\n",
    "        optimizer = hyper_parameters[\"optimizer\"](net_parameters, lr=lr)\n",
    "        \n",
    "        #create training log\n",
    "        train_log_file_name = fold_dir +\"/train_log.txt\"\n",
    "        train_log_file = open(train_log_file_name, \"w\")\n",
    "        train_log_file.write( \"Training log fold {} :\\n\".format(str(n_split)) )\n",
    "        net.show_layers()\n",
    "        train_log_file.write(str(net.get_net_struct()))\n",
    "\n",
    "        \"\"\"split training in train and val and load data\"\"\"\n",
    "        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "        val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "        #test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        #val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        #test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "\n",
    "        train_state_dir = fold_dir + \"/net_states\"\n",
    "        try:\n",
    "            os.makedirs(train_state_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        train_loss_curve = []\n",
    "        val_loss_curve = []\n",
    "        \n",
    "        fold_epoch_model_errors = []\n",
    "        fold_epoch_model_errors_path_list = []\n",
    "        fold_epoch_saved_states_path_list = []\n",
    "        for epoch in range(0, epochs):\n",
    "            \n",
    "            batch_nr = 0\n",
    "            epoch_loss = 0.\n",
    "\n",
    "            \"\"\"Actual training step\"\"\"\n",
    "            for train_mini_batch in train_loader:\n",
    "                batch_nr += 1\n",
    "                batch_loss, train_output = step(net, train_mini_batch, batch_size, loss_func, optimizer, epoch, batch_nr, device, train_log_file, mode=\"train\")\n",
    "                epoch_loss += batch_loss.item()\n",
    "            \n",
    "            \"\"\"Averaging training loss\"\"\"\n",
    "            epoch_loss = epoch_loss/len(train_loader)\n",
    "            train_loss_curve.append(epoch_loss)\n",
    "            fold_loss_curve.append(epoch_loss)\n",
    "\n",
    "            print(\"mean epoch loss: {}\".format(epoch_loss))\n",
    "            train_log_file.write(\"mean epoch loss: {}\\n\".format(epoch_loss))\n",
    "\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            if (epoch+1) in save_state_epochs or epoch == epochs:\n",
    "                train_state_epoch_file_name = \"state_epoch_{}\".format(epoch)\n",
    "                train_state = {\"epoch\" : epoch, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "                torch.save(train_state, train_state_dir + \"/\" + train_state_epoch_file_name )\n",
    "                fold_epoch_saved_states_path_list.append(train_state_dir + \"/\" + train_state_epoch_file_name)\n",
    "                print(\"saved model from epoch {}\".format(epoch))\n",
    "                train_log_file.write(\"saved model from epoch {}\\n\".format(epoch))\n",
    "        \n",
    "            \"\"\"\n",
    "            Valdation\n",
    "            \"\"\"\n",
    "            #validate for each epoch reached in val_epochs\n",
    "            if (epoch+1) in val_epochs:\n",
    "            #if (epoch) in val_epochs:\n",
    "\n",
    "                val_loss = []\n",
    "                val_pred = []\n",
    "                val_label = []\n",
    "                val_i = 0\n",
    "                for val_mini_batch in val_loader:\n",
    "\n",
    "                    feat_batch = val_mini_batch[0]\n",
    "                    label_batch = val_mini_batch[1]\n",
    "                    val_label.append(label_batch.numpy())\n",
    "                    \n",
    "                    val_batch_loss, val_output = step(net, val_mini_batch, batch_size, loss_func, optimizer, epoch, batch_nr, device, train_log_file, mode=\"val\")\n",
    "                    \n",
    "                    if hyper_parameters[\"task\"] == \"classification\":\n",
    "                        \"\"\"for a classification task\"\"\"\n",
    "                        class_batch_pred = []\n",
    "                        for output_val in val_output:\n",
    "                            class_index = output_val.argmax().detach().cpu()\n",
    "                            class_batch_pred.append(class_index)\n",
    "                            #print(output_val)\n",
    "                            #print(class_index)\n",
    "                        val_pred.append(class_batch_pred)\n",
    "                        #print(val_pred)\n",
    "                    elif hyper_parameters[\"task\"] == \"regression\":\n",
    "                        \"\"\"for a regression task\"\"\"\n",
    "                        val_pred.append(val_output.detach().cpu().numpy())\n",
    "                    \n",
    "\n",
    "                    val_loss.append(val_batch_loss.item())\n",
    "                    #print(val_batch_loss.item())\n",
    "                    val_i += 1\n",
    "                \n",
    "                \n",
    "                val_pred_df = pd.DataFrame(np.concatenate(np.asarray(val_pred)))\n",
    "                #print(np.concatenate(np.asarray(val_pred)))\n",
    "                val_label_df = pd.DataFrame(np.concatenate(np.asarray(val_label)), )\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                if type(inv_scaling_func) is not None:\n",
    "                    scaled_val_pred_df = inv_scaling_func(val_pred_df.copy())\n",
    "                    scaled_val_label_df = inv_scaling_func(val_label_df.copy())\n",
    "                    dev_df = scaled_val_label_df - scaled_val_pred_df\n",
    "                \"\"\"\n",
    "                \n",
    "                val_error_df = pd.DataFrame()\n",
    "                \n",
    "                val_error_df[\"train_label\"] = val_label_df[0]\n",
    "                val_error_df[\"train_prediction\"] = val_pred_df[0]\n",
    "                \n",
    "                \n",
    "                fold_epoch_model_errors.append(val_error_df.copy())\n",
    "                fold_epoch_model_pred_path = fold_dir + \"/\" +\"val_epoch_{}_pred\".format(epoch)\n",
    "                fold_epoch_model_labels_path = fold_dir + \"/\" +\"val_epoch_{}_labels\".format(epoch)\n",
    "                val_pred_df.to_pickle(fold_epoch_model_pred_path)\n",
    "                val_label_df.to_pickle(fold_epoch_model_labels_path)\n",
    "                fold_epoch_model_errors_path_list.append([fold_epoch_model_pred_path, fold_epoch_model_labels_path])\n",
    "        \n",
    "        \"\"\"fold_model_errors.append(fold_epoch_model_errors.copy())\"\"\"\n",
    "        fold_model_errors_path_list.append(fold_epoch_model_errors_path_list)\n",
    "        fold_saved_states_path_list.append(fold_epoch_saved_states_path_list)\n",
    "        par_loss_curves.append(fold_loss_curve)\n",
    "        \n",
    "        n_split += 1\n",
    "        \n",
    "        del net\n",
    "        del optimizer\n",
    "        \n",
    "    \"\"\"model_errors.append(fold_model_errors.copy())\"\"\"\n",
    "    model_errors_path_list.append(fold_model_errors_path_list)\n",
    "    saved_states_file_path_list.append(fold_saved_states_path_list)\n",
    "    loss_curves.append(par_loss_curves)\n",
    "    \n",
    "    \"\"\"Plot training loss curve and save as image\"\"\"\n",
    "    train_loss_img_file_name = par_dir + \"/\" + \"train_loss_par_\" + str(par_i) + \".png\"\n",
    "    x_epochs = range(epochs)\n",
    "    for fold_i in range(len(loss_curves[par_i])):\n",
    "        plt.plot(x_epochs, loss_curves[par_i][fold_i])\n",
    "    #plt.title()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"training loss\")\n",
    "    plt.savefig(train_loss_img_file_name)\n",
    "    plt.close()\n",
    "    train_loss_txt_file_name = par_dir + \"/\" + \"train_loss_par_\" + str(par_i) + \".txt\"\n",
    "    np.savetxt(train_loss_txt_file_name, loss_curves[par_i])\n",
    "    \n",
    "    par_i += 1\n",
    "    \n",
    "    train_log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "parameter_selector = cart_product(parameter_options)\n",
    "parameter_selector_list = []\n",
    "par_list_file = open(\"par_list.txt\",\"w\")\n",
    "i=0\n",
    "for cart in parameter_selector:\n",
    "    #print(cart)\n",
    "    par_list_file.write(str(i) + \"\\n\")\n",
    "    par_list_file.write(str(cart) + \"\\n\")\n",
    "    parameter_selector_list.append(cart)\n",
    "    i += 1\n",
    "print(len(parameter_selector_list))\n",
    "par_list_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#model_errors[0][0][0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_prediction</th>\n",
       "      <th>train_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_prediction  train_label\n",
       "0                 1          0.0\n",
       "1                 1          1.0\n",
       "2                 1          1.0\n",
       "3                 1          1.0\n",
       "4                 1          1.0\n",
       "5                 0          0.0\n",
       "6                 1          1.0\n",
       "7                 1          1.0\n",
       "8                 1          1.0\n",
       "9                 1          0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#fourth is prediction or label\n",
    "test_pickle_pred_df = pd.read_pickle(model_errors_path_list[0][0][-1][0])\n",
    "test_pickle_label_df = pd.read_pickle(model_errors_path_list[0][0][-1][1])\n",
    "test_pickle_label_df.columns = [\"train_label\"]\n",
    "test_pickle_pred_classes = test_pickle_pred_df.max(axis=1)\n",
    "test_pickle_pred_df[\"train_prediction\"] = pd.DataFrame(test_pickle_pred_classes)\n",
    "test_pickle_errors_df = pd.DataFrame()\n",
    "test_pickle_errors_df[\"train_prediction\"] = test_pickle_pred_df[\"train_prediction\"]\n",
    "test_pickle_errors_df[\"train_label\"] = test_pickle_label_df[\"train_label\"]\n",
    "\n",
    "test_pickle_errors_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   0   1  All\n",
      "Label                 \n",
      "0.0        16   9   25\n",
      "1.0         1  58   59\n",
      "All        17  67   84\n",
      "[[16  9 25]\n",
      " [ 1 58 59]\n",
      " [17 67 84]]\n",
      "((0, 0), (0, 0))\n",
      "(3, 3)\n",
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbulusu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For classification\n",
    "plot the confusion matrix to evaluate model\n",
    "\"\"\"\n",
    "conf_mat = pd.crosstab(test_pickle_errors_df[\"train_label\"], test_pickle_errors_df[\"train_prediction\"], rownames=[\"Label\"], colnames=[\"Predicted\"], margins=True)\n",
    "print(conf_mat)\n",
    "np_conf_mat = conf_mat.values.copy()\n",
    "print(np_conf_mat)\n",
    "conf_mat_shape = list(np_conf_mat.shape)\n",
    "\n",
    "pad_extend = ((0,0),(0,output_size-conf_mat_shape[1]+1))\n",
    "print(pad_extend)\n",
    "print(np_conf_mat.shape)\n",
    "diag_indices = [i for i in range(0,output_size)]\n",
    "diag_indices = [diag_indices, diag_indices]\n",
    "np_conf_mat = np.pad(np_conf_mat, pad_extend, mode=\"constant\", constant_values=0)\n",
    "np_conf_mat[:,-1] = np_conf_mat[:,conf_mat_shape[1]-1]\n",
    "np_conf_mat[:,conf_mat_shape[1]-1] = 0.\n",
    "#print(np_conf_mat)\n",
    "print(np_conf_mat[diag_indices].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
