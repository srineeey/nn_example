{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import itertools\n",
    "\n",
    "import PIL\n",
    "\n",
    "import dataset_class\n",
    "import torch_net_class\n",
    "\n",
    "import functools\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizing CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Utilizing CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizing CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function for training loop\n",
    "\"\"\"\n",
    "\n",
    "def step(model, input_data, batch_size, loss_func, optimizer, epoch, batch_nr, device, log_file, mode=\"val\"):\n",
    "        if mode == \"train\":\n",
    "            model.train()\n",
    "        elif mode == \"val\":\n",
    "            model.eval()\n",
    "        \n",
    "        #input data splitted into features and labels\n",
    "        feat_batch = input_data[0].to(device)\n",
    "        label_batch = input_data[1].to(device)\n",
    "        \n",
    "        input_size = model.get_input_size()\n",
    "        output_size = model.get_output_size()\n",
    "    \n",
    "        \"\"\"ALWAYS SET GRADIENT TO ZERO  FOR STANDARD NN (NOT RNNs)\"\"\"\n",
    "        model.zero_grad()\n",
    "        #optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        the peculiar shape (-1, sample_size) is needed, because an entire mini batch is passed on to the network\n",
    "        initially it is not clear how large such a mini batch is\n",
    "        the -1 acts as a placeholder in order to keep the number of processed items in one mini batch flexible\n",
    "        \"\"\"\n",
    "        #print(\"Input {}, batch: {} : {}\".format(epoch, batch_nr, feat_batch))\n",
    "        #log_file.write(\"Input {}, batch: {} : {}\\n\".format(epoch, batch_nr, feat_batch))\n",
    "        \n",
    "        #print(\"Reshaping to {}\".format( feat_batch.view(-1, input_size).double().size() ))\n",
    "        output = model(feat_batch.view(-1, input_size).float())\n",
    "        output = output.to(device)\n",
    "    \n",
    "        #print(\"Output {}, batch: {} : {}\".format(epoch, batch_nr, output))\n",
    "        #print(\"Label {}, batch: {} : {}\".format(epoch, batch_nr, label_batch))\n",
    "        #log_file.write(\"Output {}, batch: {} : {}\\n\".format(epoch, batch_nr, output))\n",
    "        \n",
    "        #print(\"Input shape {}\".format(feat_batch.size()))\n",
    "        #print(\"Label shape {}\".format(label_batch.size()))\n",
    "        #print(\"Output shape {}\".format(output.size()))\n",
    "    \n",
    "        #print(\"Feeding forward epoch: {}, batch: {}\".format(epoch, batch_nr))\n",
    "        #log_file.write(\"Feeding forward epoch: {}, batch: {}\\n\".format(epoch, batch_nr))\n",
    "    \n",
    "        #print(\"Calculating \" + mode + \" loss epoch: {}, batch: {}\".format(epoch, batch_nr))\n",
    "        #log_file.write(\"Calculating \" + mode + \" loss epoch: {}, batch: {}\\n\".format(epoch, batch_nr))\n",
    "        \"\"\"MSE\"\"\"\n",
    "        #loss = loss_func(output.view(-1, output_size).float(), label_batch.view(-1, output_size).float())\n",
    "        \"\"\"Cross Entropy\"\"\"\n",
    "        loss = loss_func(output.view(-1, output_size).float(), label_batch.view(-1).long())\n",
    "        \n",
    "        \n",
    "        if mode == \"train\":\n",
    "            #print(\"epoch: {}, batch: {}, loss: {}\".format(epoch, batch_nr, loss.item()))\n",
    "            #print(\"Performing backprop ...\")\n",
    "            #log_file.write(\"epoch: {}, batch: {}, loss: {}\\n\".format(epoch, batch_nr, loss.item()))\n",
    "            #log_file.write(\"Performing backprop ...\\n\")\n",
    "            loss.backward()\n",
    "    \n",
    "            #print(\"Adjusting weights ...\")\n",
    "            #log_file.write(\"Adjusting weights ...\\n\")\n",
    "            optimizer.step()\n",
    "    \n",
    "        return loss, output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
      "['label']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data preparation\n",
    "\"\"\"\n",
    "\n",
    "dataset_file_path = \"ionosphere_train.csv\"\n",
    "test_dataset_file_path=\"ionosphere_test.csv\"\n",
    "dataset = dataset_class.dataset(dataset_file_path)\n",
    "test_dataset = dataset_class.dataset(test_dataset_file_path)\n",
    "#selected_pred_attributes = pd.read_csv('amazon_pred_attributes_var_corr.csv', header=None)[0].tolist()\n",
    "#target_attributes = pd.read_csv('amazon_target_attribute.csv', header=None)[0].tolist()\n",
    "selected_pred_attributes = [ '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
    "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
    "       '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
    "target_attributes = [\"label\"]\n",
    "output_attributes = [\"good\", \"bad\"]\n",
    "print(selected_pred_attributes)\n",
    "print(target_attributes)\n",
    "#print(list(dataset.df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
      "['label']\n"
     ]
    }
   ],
   "source": [
    "pred_attributes = selected_pred_attributes\n",
    "\n",
    "dataset.set_pred_attributes(pred_attributes)\n",
    "dataset.set_target_attribute(target_attributes)\n",
    "test_dataset.set_pred_attributes(pred_attributes)\n",
    "test_dataset.set_target_attribute(target_attributes)\n",
    "print(pred_attributes)\n",
    "print(target_attributes)\n",
    "#dataset.df[pred_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.477824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label\n",
       "count  280.000000\n",
       "mean     0.650000\n",
       "std      0.477824\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[target_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.0</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.889286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.642006</td>\n",
       "      <td>0.005401</td>\n",
       "      <td>0.613042</td>\n",
       "      <td>0.124715</td>\n",
       "      <td>0.558642</td>\n",
       "      <td>0.100720</td>\n",
       "      <td>0.514151</td>\n",
       "      <td>0.178432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401549</td>\n",
       "      <td>-0.048062</td>\n",
       "      <td>0.546989</td>\n",
       "      <td>-0.057030</td>\n",
       "      <td>0.376318</td>\n",
       "      <td>-0.047793</td>\n",
       "      <td>0.349299</td>\n",
       "      <td>0.013766</td>\n",
       "      <td>0.358110</td>\n",
       "      <td>0.006360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.314340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507474</td>\n",
       "      <td>0.444380</td>\n",
       "      <td>0.520414</td>\n",
       "      <td>0.455321</td>\n",
       "      <td>0.493897</td>\n",
       "      <td>0.514517</td>\n",
       "      <td>0.522983</td>\n",
       "      <td>0.485672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584663</td>\n",
       "      <td>0.501739</td>\n",
       "      <td>0.530014</td>\n",
       "      <td>0.554377</td>\n",
       "      <td>0.590581</td>\n",
       "      <td>0.503374</td>\n",
       "      <td>0.591181</td>\n",
       "      <td>0.513241</td>\n",
       "      <td>0.528868</td>\n",
       "      <td>0.455715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473603</td>\n",
       "      <td>-0.082590</td>\n",
       "      <td>0.435122</td>\n",
       "      <td>-0.023300</td>\n",
       "      <td>0.256398</td>\n",
       "      <td>-0.054445</td>\n",
       "      <td>0.137793</td>\n",
       "      <td>-0.044605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.279763</td>\n",
       "      <td>0.332323</td>\n",
       "      <td>-0.336245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.240383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.234442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.188065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.880940</td>\n",
       "      <td>0.007075</td>\n",
       "      <td>0.833680</td>\n",
       "      <td>0.023915</td>\n",
       "      <td>0.748205</td>\n",
       "      <td>0.011710</td>\n",
       "      <td>0.692890</td>\n",
       "      <td>0.023530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559255</td>\n",
       "      <td>-0.008660</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>-0.014165</td>\n",
       "      <td>0.507815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.432665</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.156713</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.338090</td>\n",
       "      <td>0.968035</td>\n",
       "      <td>0.364705</td>\n",
       "      <td>0.956382</td>\n",
       "      <td>0.541510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916347</td>\n",
       "      <td>0.160588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.162285</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>0.129445</td>\n",
       "      <td>0.880257</td>\n",
       "      <td>0.214865</td>\n",
       "      <td>0.839647</td>\n",
       "      <td>0.155463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0      1           2           3           4           5  \\\n",
       "count  280.000000  280.0  280.000000  280.000000  280.000000  280.000000   \n",
       "mean     0.889286    0.0    0.642006    0.005401    0.613042    0.124715   \n",
       "std      0.314340    0.0    0.507474    0.444380    0.520414    0.455321   \n",
       "min      0.000000    0.0   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%      1.000000    0.0    0.473603   -0.082590    0.435122   -0.023300   \n",
       "50%      1.000000    0.0    0.880940    0.007075    0.833680    0.023915   \n",
       "75%      1.000000    0.0    1.000000    0.156713    1.000000    0.338090   \n",
       "max      1.000000    0.0    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "                6           7           8           9  ...          24  \\\n",
       "count  280.000000  280.000000  280.000000  280.000000  ...  280.000000   \n",
       "mean     0.558642    0.100720    0.514151    0.178432  ...    0.401549   \n",
       "std      0.493897    0.514517    0.522983    0.485672  ...    0.584663   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000  ...   -1.000000   \n",
       "25%      0.256398   -0.054445    0.137793   -0.044605  ...    0.000000   \n",
       "50%      0.748205    0.011710    0.692890    0.023530  ...    0.559255   \n",
       "75%      0.968035    0.364705    0.956382    0.541510  ...    0.916347   \n",
       "max      1.000000    1.000000    1.000000    1.000000  ...    1.000000   \n",
       "\n",
       "               25          26          27          28          29          30  \\\n",
       "count  280.000000  280.000000  280.000000  280.000000  280.000000  280.000000   \n",
       "mean    -0.048062    0.546989   -0.057030    0.376318   -0.047793    0.349299   \n",
       "std      0.501739    0.530014    0.554377    0.590581    0.503374    0.591181   \n",
       "min     -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   -1.000000   \n",
       "25%     -0.279763    0.332323   -0.336245    0.000000   -0.240383    0.000000   \n",
       "50%     -0.008660    0.723380   -0.014165    0.507815    0.000000    0.451060   \n",
       "75%      0.160588    1.000000    0.162285    0.904700    0.129445    0.880257   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               31          32          33  \n",
       "count  280.000000  280.000000  280.000000  \n",
       "mean     0.013766    0.358110    0.006360  \n",
       "std      0.513241    0.528868    0.455715  \n",
       "min     -1.000000   -1.000000   -1.000000  \n",
       "25%     -0.234442    0.000000   -0.188065  \n",
       "50%      0.000000    0.432665    0.000000  \n",
       "75%      0.214865    0.839647    0.155463  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df[pred_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data scaling'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Data scaling\"\"\"\n",
    "\n",
    "#dataset.min_max_scaling(attributes=pred_attributes)\n",
    "#dataset.z_score_standardization(attributes=pred_attributes)\n",
    "#test_dataset.z_score_standardization(attributes=pred_attributes)\n",
    "#dataset.z_score_standardization(attributes=pred_attributes[3:])\n",
    "#dataset.min_max_scaling(attributes=target_attributes)\n",
    "#scaling_func=lambda x: x/9.\n",
    "#inv_scaling_func = lambda x: 9.*x\n",
    "#dataset.custom_scaling(attributes=target_attributes, scaling_func=scaling_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.df[pred_attributes].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.df[target_attributes].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "HYPERPARAMETERS\n",
    "\"\"\"\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "\"\"\"\n",
    "Some possible model parameters:\n",
    "parameter_options = {}\n",
    "parameter_options[\"act_func\"] = [torch.relu, torch.sigmoid, , torch.tanh]\n",
    "parameter_options[\"loss_func\"] = [torch.sigmoid, torch.relu, torch.tanh]\n",
    "parameter_options[\"dropout\"] = [nn.Dropout, None]\n",
    "parameter_options[\"optimizer\"] = [optim.Adam, optim.SGD]\n",
    "parameter_options[\"val\"] = [\"kfold\", \"holdout\"]\n",
    "parameter_options[\"task\"] = [\"classification\", \"regression\"] \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"the parameter options dictionary records which hyperparameters to use\"\"\"\n",
    "parameter_options = {}\n",
    "\"\"\"\n",
    "parameter_options[\"task\"] = [\"classification\"] \n",
    "\n",
    "parameter_options[\"val\"] = [\"holdout\"]\n",
    "parameter_options[\"split\"] = [0.7]\n",
    "#parameter_options[\"val\"] = [\"kfold\"]\n",
    "#parameter_options[\"k_fold\"] = [10]\n",
    "\n",
    "parameter_options[\"batch_size\"] = [1]\n",
    "parameter_options[\"lr\"] = [0.001]\n",
    "\n",
    "parameter_options[\"hidden_layers\"] = [3]\n",
    "parameter_options[\"layer_width\"] = [9]\n",
    "parameter_options[\"bias\"] = [True]\n",
    "parameter_options[\"act_func\"] = [torch.relu]\n",
    "\n",
    "#parameter_options[\"loss_func\"] = [nn.MSELoss]\n",
    "parameter_options[\"loss_func\"] = [nn.CrossEntropyLoss]\n",
    "parameter_options[\"optimizer\"] = [optim.Adam]\n",
    "\n",
    "parameter_options[\"dropout\"] = [None]\n",
    "parameter_options[\"p_dropout\"] = [0.1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Create cartesian product of all possible parameter combinations\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def cart_product(dict_options):\n",
    "    return ( dict(zip(dict_options.keys(), values)) for values in itertools.product(*dict_options.values()) )\n",
    "\n",
    "parameter_selector = cart_product(parameter_options)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Manually set network structure\"\"\"\n",
    "\"\"\"\n",
    "    This list can be loaded into the constructor of the Net neural network class, to automatically generate the network structure\n",
    "    type = pointer to the layer function'\n",
    "    layer_pars = parameters which must be given to the layer function in order to initialize it\n",
    "    act_func = activation function to be applied directly after feeding to the corresponding layer\n",
    "    dropout = certain neurons cna be dropped out if specified\n",
    "\"\"\"\n",
    "\n",
    "fixed_net_struct = []\n",
    "layer = nn.Linear\n",
    "act_func = torch.relu\n",
    "dropout = nn.Dropout\n",
    "#dropout = None\n",
    "p = 0.2\n",
    "bias = True\n",
    "input_size = len(pred_attributes)\n",
    "target_size = len(target_attributes)\n",
    "output_size = len(output_attributes)\n",
    "fixed_net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": input_size, \"out_features\": 20}, \"act_func\": act_func, \"bias\": bias} )\n",
    "if dropout is not None:    \n",
    "    fixed_net_struct.append( {\"type\": dropout, \"layer_pars\": {\"p\": p }} )\n",
    "fixed_net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": 20, \"out_features\": 20}, \"act_func\": act_func, \"bias\": bias} )\n",
    "fixed_net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": 20, \"out_features\": output_size}, \"bias\": bias} )\n",
    "\n",
    "\"\"\"If required create list of parameters manually\"\"\"\n",
    "\n",
    "parameter_selector = []\n",
    "\n",
    "parameter_option = {\"batch_size\":16,\"lr\":0.0001,\"loss_func\":nn.CrossEntropyLoss,\"optimizer\":optim.Adam}\n",
    "\n",
    "parameter_option[\"task\"] = \"classification\"\n",
    "\n",
    "parameter_option[\"val\"] = \"holdout\"\n",
    "parameter_option[\"split_ratio\"] = {\"train\" : 0.7, \"val\" : 0.3, \"test\" : 0.0}\n",
    "\n",
    "#parameter_option[\"val\"] = \"kfold\"\n",
    "#parameter_option[\"ksplits\"] = 4\n",
    "\n",
    "parameter_option[\"hidden_layers\"] = 3\n",
    "parameter_option[\"layer_width\"] = 10\n",
    "parameter_option[\"act_func\"] = torch.relu\n",
    "parameter_option[\"net_struct\"] = fixed_net_struct\n",
    "parameter_selector.append(parameter_option)\n",
    "\n",
    "\"Set the number of epochs (max(val_epochs) and at which epochs model shall be validated)\"\n",
    "val_epochs = [16, 32, 48, 64, 86, 100]\n",
    "#val_epochs = [16, 32, 48]\n",
    "#val_epochs = [5]\n",
    "save_state_epochs = [100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMAYBE AN ADDITIONAL TRAIN TEST SPLIT IS NECESSARY HERE\\nNEXT CELL = TRAIN + VAL\\nAFTER PLOTTING CONF MAT -> TESTING\\nFor splitting add line:\\nremaining_indices, test_indices = train_test_split(all_indices, test_size=test_size, shuffle=True, random_state=random_seed)\\n\\nLoading this separate test split for evaluation then either required loading test_indices:\\ntest_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\\ntest_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\\n\\n\\nThe Code which splits train and val is written as (see below):\\nall_indices = remaining_indices\\nindices[0], indices[1] = train_test_split(all_indices, test_size=split_ratio[\"val\"], shuffle=True, random_state=random_seed)\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "MAYBE AN ADDITIONAL TRAIN TEST SPLIT IS NECESSARY HERE\n",
    "NEXT CELL = TRAIN + VAL\n",
    "AFTER PLOTTING CONF MAT -> TESTING\n",
    "For splitting add line:\n",
    "remaining_indices, test_indices = train_test_split(all_indices, test_size=test_size, shuffle=True, random_state=random_seed)\n",
    "\n",
    "Loading this separate test split for evaluation then either required loading test_indices:\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "\n",
    "The Code which splits train and val is written as (see below):\n",
    "all_indices = remaining_indices\n",
    "indices[0], indices[1] = train_test_split(all_indices, test_size=split_ratio[\"val\"], shuffle=True, random_state=random_seed)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "all_indices = list(range(dataset.get_length()))\n",
    "remaining_indices, test_indices = train_test_split(all_indices, test_size=parameter_option[\"split\"][\"test\"], shuffle=True, random_state=random_seed)\n",
    "\n",
    "print(\"size of test set :{}\".format(int(parameter_option[\"split\"][\"test\"]*dataset.get_length())))\n",
    "print(\"size of train set :{}\".format(int(parameter_option[\"split\"][\"train\"]*dataset.get_length())))\n",
    "print(\"one batch with size:{}\".format(int(parameter_option[\"split\"][\"train\"]*dataset.get_length())%parameter_option[\"batch_size\"]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 34, 'out_features': 20}, 'act_func': <built-in method relu of type object at 0x7ff2b2cb6be0>, 'bias': True}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.dropout.Dropout'>, 'layer_pars': {'p': 0.2}}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 20, 'out_features': 20}, 'act_func': <built-in method relu of type object at 0x7ff2b2cb6be0>, 'bias': True}\n",
      "\n",
      "Adding {'type': <class 'torch.nn.modules.linear.Linear'>, 'layer_pars': {'in_features': 20, 'out_features': 2}, 'bias': True}\n",
      "\n",
      "Linear(in_features=34, out_features=20, bias=True)\n",
      "Dropout(p=0.2, inplace=False)\n",
      "Linear(in_features=20, out_features=20, bias=True)\n",
      "Linear(in_features=20, out_features=2, bias=True)\n",
      "mean epoch loss: 0.7622593962229215\n",
      "mean epoch loss: 0.7420881298872141\n",
      "mean epoch loss: 0.7445767109210675\n",
      "mean epoch loss: 0.7143227687248817\n",
      "mean epoch loss: 0.6827638607758743\n",
      "mean epoch loss: 0.6760291182077848\n",
      "mean epoch loss: 0.6585495059306805\n",
      "mean epoch loss: 0.639196868126209\n",
      "mean epoch loss: 0.6655791080915011\n",
      "mean epoch loss: 0.6359039728458111\n",
      "mean epoch loss: 0.6310796783520625\n",
      "mean epoch loss: 0.6198254731985239\n",
      "mean epoch loss: 0.6278086442213792\n",
      "mean epoch loss: 0.6050154612614558\n",
      "mean epoch loss: 0.6104837105824397\n",
      "mean epoch loss: 0.6003280098621662\n",
      "mean epoch loss: 0.5917086945130274\n",
      "mean epoch loss: 0.5721374291640061\n",
      "mean epoch loss: 0.5691467248476468\n",
      "mean epoch loss: 0.5539259062363551\n",
      "mean epoch loss: 0.5558945559538327\n",
      "mean epoch loss: 0.5827265427662776\n",
      "mean epoch loss: 0.5599488180417281\n",
      "mean epoch loss: 0.5537347633105058\n",
      "mean epoch loss: 0.5615392694106469\n",
      "mean epoch loss: 0.5430083664563986\n",
      "mean epoch loss: 0.5372499777720525\n",
      "mean epoch loss: 0.5609933665165534\n",
      "mean epoch loss: 0.5381087912962987\n",
      "mean epoch loss: 0.5533470878234277\n",
      "mean epoch loss: 0.5135762851971847\n",
      "mean epoch loss: 0.5224801989702078\n",
      "mean epoch loss: 0.5256014603834885\n",
      "mean epoch loss: 0.5296216630018674\n",
      "mean epoch loss: 0.5375879429853879\n",
      "mean epoch loss: 0.5213683110017043\n",
      "mean epoch loss: 0.5263183965132787\n",
      "mean epoch loss: 0.5131725806456345\n",
      "mean epoch loss: 0.49685619198358977\n",
      "mean epoch loss: 0.5021811975882604\n",
      "mean epoch loss: 0.5047606642429645\n",
      "mean epoch loss: 0.4917659805371211\n",
      "mean epoch loss: 0.49075751121227557\n",
      "mean epoch loss: 0.5000653610779688\n",
      "mean epoch loss: 0.4720937403348776\n",
      "mean epoch loss: 0.4859644266275259\n",
      "mean epoch loss: 0.47649219173651475\n",
      "mean epoch loss: 0.5031977020777189\n",
      "mean epoch loss: 0.5175789732199448\n",
      "mean epoch loss: 0.47109132088147676\n",
      "mean epoch loss: 0.48140567999619704\n",
      "mean epoch loss: 0.4811305128611051\n",
      "mean epoch loss: 0.4664319661947397\n",
      "mean epoch loss: 0.4986502253092252\n",
      "mean epoch loss: 0.4718383779892555\n",
      "mean epoch loss: 0.45317445122278655\n",
      "mean epoch loss: 0.4522599532054021\n",
      "mean epoch loss: 0.44457138730929446\n",
      "mean epoch loss: 0.4549171970440791\n",
      "mean epoch loss: 0.4314195949297685\n",
      "mean epoch loss: 0.45344301141225374\n",
      "mean epoch loss: 0.46107690380169797\n",
      "mean epoch loss: 0.44531060411379886\n",
      "mean epoch loss: 0.4498098584321829\n",
      "mean epoch loss: 0.4293015988973471\n",
      "mean epoch loss: 0.4414600913341229\n",
      "mean epoch loss: 0.4326486381200644\n",
      "mean epoch loss: 0.4282000660896301\n",
      "mean epoch loss: 0.4392553361562582\n",
      "mean epoch loss: 0.39513108363518346\n",
      "mean epoch loss: 0.4280505891029651\n",
      "mean epoch loss: 0.4261363996909215\n",
      "mean epoch loss: 0.4143595305772928\n",
      "mean epoch loss: 0.41903193868123567\n",
      "mean epoch loss: 0.3929191804849185\n",
      "mean epoch loss: 0.4077614889695094\n",
      "mean epoch loss: 0.39485331108936894\n",
      "mean epoch loss: 0.4215600605194385\n",
      "mean epoch loss: 0.4014512850688054\n",
      "mean epoch loss: 0.401630862401082\n",
      "mean epoch loss: 0.39671256565130675\n",
      "mean epoch loss: 0.40601948591379017\n",
      "mean epoch loss: 0.4200755667227965\n",
      "mean epoch loss: 0.43279513028951794\n",
      "mean epoch loss: 0.3803713390460381\n",
      "mean epoch loss: 0.4000357733323024\n",
      "mean epoch loss: 0.37743213085027844\n",
      "mean epoch loss: 0.4034049625580127\n",
      "mean epoch loss: 0.3923002458535708\n",
      "mean epoch loss: 0.4026214824273036\n",
      "mean epoch loss: 0.37010837632876176\n",
      "mean epoch loss: 0.4071573821397928\n",
      "mean epoch loss: 0.37938724343593305\n",
      "mean epoch loss: 0.3954301659877484\n",
      "mean epoch loss: 0.35809271381451535\n",
      "mean epoch loss: 0.36428494636829084\n",
      "mean epoch loss: 0.37089591989150417\n",
      "mean epoch loss: 0.3555534321528215\n",
      "mean epoch loss: 0.3919729429941911\n",
      "mean epoch loss: 0.3714046168785829\n",
      "saved model from epoch 99\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "All errors from each validation epoch will be saved\n",
    "this list contains them\n",
    "Saving them in memory is not always viable, which is why they are also saved on the hard drive\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "model_errors = []\n",
    "\"\"\"\n",
    "All errors from each validation epoch will be saved as a file\n",
    "this list contains the path to these files\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#fourth is prediction or label \n",
    "model_errors_path_list = []\n",
    "\n",
    "\"\"\"\n",
    "The average training loss for each epoch is recorded here\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "loss_curves = []\n",
    "loss_path_list = []\n",
    "\n",
    "\"\"\"\n",
    "States of the neural network are saved as a file\n",
    "filename is part of this list\n",
    "\"\"\"\n",
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "saved_states_file_path_list = []\n",
    "\n",
    "\"\"\"Perform training for each parameter option in parameter_selector\"\"\"\n",
    "par_i = 0\n",
    "for hyper_parameters in parameter_selector:\n",
    "    \n",
    "    epochs = max(val_epochs)\n",
    "    input_size = len(pred_attributes)\n",
    "    target_size = len(target_attributes)\n",
    "    output_size = len(output_attributes)\n",
    "    \n",
    "    lr=hyper_parameters[\"lr\"]\n",
    "    batch_size = hyper_parameters[\"batch_size\"]\n",
    "    \n",
    "    loss_func = hyper_parameters[\"loss_func\"]()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    If a network structure is fixed beforehand, construct this\n",
    "    Otherwise create a \"rectangular\" fully connected feedforward network, with a fixed number of hidden layers and neurons per layer\n",
    "    \"\"\"\n",
    "    net_struct = []\n",
    "    if \"net_struct\" in hyper_parameters.keys():\n",
    "        net_struct = hyper_parameters[\"net_struct\"]\n",
    "    else:\n",
    "        layer = nn.Linear\n",
    "        act_func = hyper_parameters[\"act_func\"]\n",
    "        dropout = hyper_parameters[\"dropout\"]\n",
    "        p = hyper_parameters[\"p_dropout\"]\n",
    "        bias = hyper_parameters[\"bias\"]\n",
    "        lw = hyper_parameters[\"layer_width\"]\n",
    "        hl = hyper_parameters[\"hidden_layers\"]\n",
    "    \n",
    "        net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": input_size, \"out_features\": lw}, \"act_func\": act_func, \"bias\": bias} )\n",
    "        if dropout is not None:    \n",
    "            net_struct.append( {\"type\": dropout, \"layer_pars\": {\"p\": p }} )\n",
    "        if hl > 1:\n",
    "            for num_layer in range(hl-1):\n",
    "                    net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": lw, \"out_features\": lw}, \"act_func\": act_func, \"bias\": bias} )\n",
    "        net_struct.append( {\"type\": layer, \"layer_pars\": {\"in_features\": lw, \"out_features\": output_size}, \"bias\": bias} )\n",
    "    \n",
    "    \"\"\"Choose between holdout validation and k-fold cross validation\"\"\"\n",
    "    if hyper_parameters[\"val\"] == \"holdout\":\n",
    "        \"\"\"DEPENDING ON TRAIN+VAL+TEST SPLIT ADAPT all_indices\"\"\"\n",
    "        all_indices = list(range(dataset.get_length()))\n",
    "        #all_indices = remaining_indices\n",
    "        indices = [None, None]\n",
    "        split_ratio = hyper_parameters[\"split_ratio\"]\n",
    "        indices[0], indices[1] = train_test_split(all_indices, test_size=split_ratio[\"val\"], shuffle=True, random_state=random_seed)\n",
    "        indices = [indices]\n",
    "    elif hyper_parameters[\"val\"] == \"kfold\":\n",
    "        \"\"\"NOT WORKING ATM\"\"\"\n",
    "        k_splits = hyper_parameters[\"ksplits\"]\n",
    "        kf = KFold(n_splits=k_splits, shuffle=True, random_state=random_seed)\n",
    "        indices = kf.split(dataset.df)\n",
    "    \n",
    "    \"\"\"\n",
    "    Training\n",
    "    \"\"\"\n",
    "    par_dir = str(\"par_{}\".format(par_i))\n",
    "    \n",
    "    fold_model_errors = []\n",
    "    fold_model_errors_path_list = []\n",
    "    fold_saved_states_path_list = []\n",
    "    par_loss_curves = []\n",
    "    n_split = 0\n",
    "    for fold_indices in indices:\n",
    "        \n",
    "        train_indices = fold_indices[0]\n",
    "        val_indices = fold_indices[1]\n",
    "        \n",
    "        fold_loss_curve = []\n",
    "        print(\"fold {}\".format(n_split))\n",
    "        fold_dir = par_dir + \"/\" + \"fold_{}\".format(str(n_split))\n",
    "        try:\n",
    "            os.makedirs(fold_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        net = torch_net_class.Net(net_struct, input_size)\n",
    "        net.init_weights(torch.nn.init.xavier_normal_)\n",
    "        net.set_batch_size(batch_size)\n",
    "        #net.cuda()\n",
    "        net.to(device)\n",
    "        \n",
    "        net_parameters = net.parameters()\n",
    "        optimizer = hyper_parameters[\"optimizer\"](net_parameters, lr=lr)\n",
    "        \n",
    "        #create training log\n",
    "        train_log_file_name = fold_dir +\"/train_log.txt\"\n",
    "        train_log_file = open(train_log_file_name, \"w\")\n",
    "        train_log_file.write( \"Training log fold {} :\\n\".format(str(n_split)) )\n",
    "        net.show_layers()\n",
    "        train_log_file.write(str(net.get_net_struct()))\n",
    "\n",
    "        \"\"\"split training in train and val and load data\"\"\"\n",
    "        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "        val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_indices)\n",
    "        #test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        #val_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "        #test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "\n",
    "        train_state_dir = fold_dir + \"/net_states\"\n",
    "        try:\n",
    "            os.makedirs(train_state_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        train_loss_curve = []\n",
    "        val_loss_curve = []\n",
    "        \n",
    "        fold_epoch_model_errors = []\n",
    "        fold_epoch_model_errors_path_list = []\n",
    "        fold_epoch_saved_states_path_list = []\n",
    "        for epoch in range(0, epochs):\n",
    "            \n",
    "            batch_nr = 0\n",
    "            epoch_loss = 0.\n",
    "\n",
    "            \"\"\"Actual training step\"\"\"\n",
    "            for train_mini_batch in train_loader:\n",
    "                batch_nr += 1\n",
    "                batch_loss, train_output = step(net, train_mini_batch, batch_size, loss_func, optimizer, epoch, batch_nr, device, train_log_file, mode=\"train\")\n",
    "                epoch_loss += batch_loss.item()\n",
    "            \n",
    "            \"\"\"Averaging training loss\"\"\"\n",
    "            epoch_loss = epoch_loss/len(train_loader)\n",
    "            train_loss_curve.append(epoch_loss)\n",
    "            fold_loss_curve.append(epoch_loss)\n",
    "\n",
    "            print(\"mean epoch loss: {}\".format(epoch_loss))\n",
    "            train_log_file.write(\"mean epoch loss: {}\\n\".format(epoch_loss))\n",
    "\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            if (epoch+1) in save_state_epochs or epoch == epochs:\n",
    "                train_state_epoch_file_name = \"state_epoch_{}\".format(epoch)\n",
    "                train_state = {\"epoch\" : epoch, \"state_dict\": net.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "                torch.save(train_state, train_state_dir + \"/\" + train_state_epoch_file_name )\n",
    "                fold_epoch_saved_states_path_list.append(train_state_dir + \"/\" + train_state_epoch_file_name)\n",
    "                print(\"saved model from epoch {}\".format(epoch))\n",
    "                train_log_file.write(\"saved model from epoch {}\\n\".format(epoch))\n",
    "        \n",
    "            \"\"\"\n",
    "            Valdation\n",
    "            \"\"\"\n",
    "            #validate for each epoch reached in val_epochs\n",
    "            if (epoch+1) in val_epochs:\n",
    "            #if (epoch) in val_epochs:\n",
    "\n",
    "                val_loss = []\n",
    "                val_pred = []\n",
    "                val_label = []\n",
    "                val_i = 0\n",
    "                for val_mini_batch in val_loader:\n",
    "\n",
    "                    feat_batch = val_mini_batch[0]\n",
    "                    label_batch = val_mini_batch[1]\n",
    "                    val_label.append(label_batch.numpy())\n",
    "                    \n",
    "                    val_batch_loss, val_output = step(net, val_mini_batch, batch_size, loss_func, optimizer, epoch, batch_nr, device, train_log_file, mode=\"val\")\n",
    "                    \n",
    "                    if hyper_parameters[\"task\"] == \"classification\":\n",
    "                        \"\"\"for a classification task\"\"\"\n",
    "                        class_batch_pred = []\n",
    "                        for output_val in val_output:\n",
    "                            class_index = output_val.argmax().detach().cpu()\n",
    "                            class_batch_pred.append(class_index)\n",
    "                            #print(output_val)\n",
    "                            #print(class_index)\n",
    "                        val_pred.append(class_batch_pred)\n",
    "                        #print(val_pred)\n",
    "                    elif hyper_parameters[\"task\"] == \"regression\":\n",
    "                        \"\"\"for a regression task\"\"\"\n",
    "                        val_pred.append(val_output.detach().cpu().numpy())\n",
    "                    \n",
    "\n",
    "                    val_loss.append(val_batch_loss.item())\n",
    "                    #print(val_batch_loss.item())\n",
    "                    val_i += 1\n",
    "                \n",
    "                \n",
    "                val_pred_df = pd.DataFrame(np.concatenate(np.asarray(val_pred)))\n",
    "                #print(np.concatenate(np.asarray(val_pred)))\n",
    "                val_label_df = pd.DataFrame(np.concatenate(np.asarray(val_label)), )\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                if type(inv_scaling_func) is not None:\n",
    "                    scaled_val_pred_df = inv_scaling_func(val_pred_df.copy())\n",
    "                    scaled_val_label_df = inv_scaling_func(val_label_df.copy())\n",
    "                    dev_df = scaled_val_label_df - scaled_val_pred_df\n",
    "                \"\"\"\n",
    "                \n",
    "                val_error_df = pd.DataFrame()\n",
    "                \n",
    "                val_error_df[\"train_label\"] = val_label_df[0]\n",
    "                val_error_df[\"train_prediction\"] = val_pred_df[0]\n",
    "                \n",
    "                \n",
    "                fold_epoch_model_errors.append(val_error_df.copy())\n",
    "                fold_epoch_model_pred_path = fold_dir + \"/\" +\"val_epoch_{}_pred\".format(epoch)\n",
    "                fold_epoch_model_labels_path = fold_dir + \"/\" +\"val_epoch_{}_labels\".format(epoch)\n",
    "                val_pred_df.to_pickle(fold_epoch_model_pred_path)\n",
    "                val_label_df.to_pickle(fold_epoch_model_labels_path)\n",
    "                fold_epoch_model_errors_path_list.append([fold_epoch_model_pred_path, fold_epoch_model_labels_path])\n",
    "        \n",
    "        \"\"\"fold_model_errors.append(fold_epoch_model_errors.copy())\"\"\"\n",
    "        fold_model_errors_path_list.append(fold_epoch_model_errors_path_list)\n",
    "        fold_saved_states_path_list.append(fold_epoch_saved_states_path_list)\n",
    "        par_loss_curves.append(fold_loss_curve)\n",
    "        \n",
    "        n_split += 1\n",
    "        \n",
    "        #del net\n",
    "        #del optimizer\n",
    "        \n",
    "    \"\"\"model_errors.append(fold_model_errors.copy())\"\"\"\n",
    "    model_errors_path_list.append(fold_model_errors_path_list)\n",
    "    saved_states_file_path_list.append(fold_saved_states_path_list)\n",
    "    loss_curves.append(par_loss_curves)\n",
    "    \n",
    "    \"\"\"Plot training loss curve and save as image\"\"\"\n",
    "    train_loss_img_file_name = par_dir + \"/\" + \"train_loss_par_\" + str(par_i) + \".png\"\n",
    "    x_epochs = range(epochs)\n",
    "    for fold_i in range(len(loss_curves[par_i])):\n",
    "        plt.plot(x_epochs, loss_curves[par_i][fold_i])\n",
    "    #plt.title()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"training loss\")\n",
    "    plt.savefig(train_loss_img_file_name)\n",
    "    plt.close()\n",
    "    train_loss_txt_file_name = par_dir + \"/\" + \"train_loss_par_\" + str(par_i) + \".txt\"\n",
    "    np.savetxt(train_loss_txt_file_name, loss_curves[par_i])\n",
    "    \n",
    "    par_i += 1\n",
    "    \n",
    "    train_log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nparameter_selector = cart_product(parameter_options)\\nparameter_selector_list = []\\npar_list_file = open(\"par_list.txt\",\"w\")\\ni=0\\nfor cart in parameter_selector:\\n    #print(cart)\\n    par_list_file.write(str(i) + \"\\n\")\\n    par_list_file.write(str(cart) + \"\\n\")\\n    parameter_selector_list.append(cart)\\n    i += 1\\nprint(len(parameter_selector_list))\\npar_list_file.close()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "parameter_selector = cart_product(parameter_options)\n",
    "parameter_selector_list = []\n",
    "par_list_file = open(\"par_list.txt\",\"w\")\n",
    "i=0\n",
    "for cart in parameter_selector:\n",
    "    #print(cart)\n",
    "    par_list_file.write(str(i) + \"\\n\")\n",
    "    par_list_file.write(str(cart) + \"\\n\")\n",
    "    parameter_selector_list.append(cart)\n",
    "    i += 1\n",
    "print(len(parameter_selector_list))\n",
    "par_list_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#model_errors[0][0][0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_prediction</th>\n",
       "      <th>train_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_prediction  train_label\n",
       "0                 1          0.0\n",
       "1                 1          1.0\n",
       "2                 1          1.0\n",
       "3                 1          1.0\n",
       "4                 0          0.0\n",
       "5                 1          1.0\n",
       "6                 0          0.0\n",
       "7                 1          1.0\n",
       "8                 1          1.0\n",
       "9                 0          0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first index is parameter run\n",
    "#second is fold\n",
    "#third is epoch\n",
    "#fourth is prediction or label\n",
    "test_pickle_pred_df = pd.read_pickle(model_errors_path_list[0][0][-1][0])\n",
    "test_pickle_label_df = pd.read_pickle(model_errors_path_list[0][0][-1][1])\n",
    "test_pickle_label_df.columns = [\"train_label\"]\n",
    "test_pickle_pred_classes = test_pickle_pred_df.max(axis=1)\n",
    "test_pickle_pred_df[\"train_prediction\"] = pd.DataFrame(test_pickle_pred_classes)\n",
    "test_pickle_errors_df = pd.DataFrame()\n",
    "test_pickle_errors_df[\"train_prediction\"] = test_pickle_pred_df[\"train_prediction\"]\n",
    "test_pickle_errors_df[\"train_label\"] = test_pickle_label_df[\"train_label\"]\n",
    "\n",
    "test_pickle_errors_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17 14]\n",
      " [ 1 52]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For classification\n",
    "plot the confusion matrix to evaluate model\n",
    "\"\"\"\n",
    "\n",
    "#label_dict = {}\n",
    "#for i in range(len(dataset.label_encoder.classes_)):\n",
    "#    label_dict[i] = dataset.label_encoder.classes_[i]\n",
    "\n",
    "#test_pickle_errors_df_copy = test_pickle_errors_df.query(\"train_prediction != 3\")\n",
    "test_pickle_errors_df_copy = test_pickle_errors_df.copy()\n",
    "sk_conf_mat = confusion_matrix(test_pickle_errors_df_copy[\"train_label\"], test_pickle_errors_df_copy[\"train_prediction\"])\n",
    "print(sk_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sk_conf_mat)\n",
    "norm_conf_mat = sk_conf_mat.astype(float)\n",
    "#print(norm_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff2444fce10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i,line in enumerate(norm_conf_mat[:,]):\n",
    "    #print(i)\n",
    "    #print(line)\n",
    "    #print(norm_conf_mat[i].sum())\n",
    "    norm_conf_mat[i] = norm_conf_mat[i]/(norm_conf_mat[i].sum())\n",
    "    \n",
    "#print(norm_conf_mat)\n",
    "sns.heatmap(norm_conf_mat,cmap=\"YlGn\",annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_label = np.array(test_pickle_errors_df[\"train_label\"])\n",
    "np_pred = np.array(test_pickle_errors_df[\"train_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      " 0.8214285714285714\n",
      "Recall:\n",
      " [0.5483871  0.98113208]\n",
      "averaged recall: 0.7647595861229458 +- 0.2163724893487523\n",
      "Precision:\n",
      " [0.94444444 0.78787879]\n",
      "averaged precision: 0.8661616161616161 +- 0.07828282828282829\n"
     ]
    }
   ],
   "source": [
    "acc = sklearn.metrics.accuracy_score(np_label, np_pred)\n",
    "rec = sklearn.metrics.recall_score(np_label, np_pred, average=None)\n",
    "prec = sklearn.metrics.precision_score(np_label, np_pred, average=None)\n",
    "\n",
    "print(\"Accuracy:\\n {}\".format(acc))\n",
    "\n",
    "print(\"Recall:\\n {}\".format(rec))\n",
    "print(\"averaged recall: {} +- {}\".format(rec.mean(), rec.std()) )\n",
    "\n",
    "print(\"Precision:\\n {}\".format(prec))\n",
    "print(\"averaged precision: {} +- {}\".format(prec.mean(), prec.std()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLoad the test data (depending in your need)\\nby loading the indices (mentioned above)\\nor creating new instance of dataset class\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Load the test data (depending in your need)\n",
    "by loading the indices (mentioned above)\n",
    "or creating new instance of dataset class\n",
    "\"\"\"\n",
    "#test_dataset = dataset_class.dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-a56fb8fe10ea>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-a56fb8fe10ea>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=)\u001b[0m\n\u001b[0m                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#test model with test dataset\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=)\n",
    "\n",
    "test_pred = []\n",
    "test_label = []\n",
    "epoch = 0\n",
    "batch_nr = 0\n",
    "for test_mini_batch in test_loader:\n",
    "    batch_nr += 1\n",
    "    feat_batch = test_mini_batch[0]\n",
    "    label_batch = test_mini_batch[1]\n",
    "    test_label.append(label_batch.detach().cpu().numpy())\n",
    "    test_batch_loss, test_output = step(net, test_mini_batch, batch_size, loss_func, optimizer, epoch, batch_nr, device, train_log_file, mode=\"test\")\n",
    "\n",
    "    if hyper_parameters[\"task\"] == \"classification\":\n",
    "        \"\"\"for a classification task\"\"\"\n",
    "        class_batch_pred = []\n",
    "        for test in test_output:\n",
    "            class_index = test.argmax().detach().cpu()\n",
    "            class_batch_pred.append(class_index)\n",
    "        test_pred.append(class_batch_pred)\n",
    "    elif hyper_parameters[\"task\"] == \"regression\":\n",
    "        \"\"\"for a regression task\"\"\"\n",
    "        test_pred.append(test_output.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\"\"\"SAVE NUMPY ARRAY INSTEAD OF DF\"\"\"\n",
    "#test_pred_df = pd.DataFrame(np.hstack(test_pred))\n",
    "#test_label_df = pd.DataFrame(np.hstack(test_label))\n",
    "\n",
    "test_pred_df = pd.DataFrame(functools.reduce(operator.iconcat, test_pred, []))\n",
    "test_label_df = pd.DataFrame(functools.reduce(operator.iconcat, test_label, []))\n",
    "\n",
    "\n",
    "test_error_df = pd.DataFrame()\n",
    "\n",
    "test_error_df[\"train_label\"] = test_label_df[0]\n",
    "test_error_df[\"train_prediction\"] = test_pred_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_conf_mat = confusion_matrix(test_error_df[\"train_label\"], test_error_df[\"train_prediction\"])\n",
    "#print(sk_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_conf_mat = sk_conf_mat.astype(float)\n",
    "#print(norm_conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in enumerate(norm_conf_mat[:,]):\n",
    "    #print(i)\n",
    "    #print(line)\n",
    "    #print(norm_conf_mat[i].sum())\n",
    "    norm_conf_mat[i] = norm_conf_mat[i]/(norm_conf_mat[i].sum())\n",
    "    \n",
    "#print(norm_conf_mat)\n",
    "sns.heatmap(norm_conf_mat,cmap=\"YlGn\",annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_label = np.array(test_pickle_errors_df[\"train_label\"])\n",
    "np_pred = np.array(test_pickle_errors_df[\"train_prediction\"])\n",
    "\n",
    "acc = sklearn.metrics.accuracy_score(np_label, np_pred)\n",
    "rec = sklearn.metrics.recall_score(np_label, np_pred, average=None)\n",
    "prec = sklearn.metrics.precision_score(np_label, np_pred, average=None)\n",
    "\n",
    "print(\"Accuracy:\\n {}\".format(acc))\n",
    "\n",
    "print(\"Recall:\\n {}\".format(rec))\n",
    "print(\"averaged recall: {} +- {}\".format(rec.mean(), rec.std()) )\n",
    "\n",
    "print(\"Precision:\\n {}\".format(prec))\n",
    "print(\"averaged precision: {} +- {}\".format(prec.mean(), prec.std()) )\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
